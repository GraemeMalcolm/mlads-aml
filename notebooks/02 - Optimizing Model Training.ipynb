{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Optimizing Model Training\n",
    "\n",
    "The benefit of cloud compute is that it offers a cost-effective way to scale out your experiment workflow and try different algorithms and parameters in order to optimize your model's performance; and that's what we'll explore in this exercise.\n",
    "\n",
    "> **Important**: This exercise assumes you have completed the previous exercises in this series - specifically, you must have:\n",
    ">\n",
    "> - Created an Azure ML Workspace.\n",
    "> - Uploaded the diabetes*.csv data files to the workspace's default datastore.\n",
    "> - Registered a **diabetes dataset** dataset in the workspace.\n",
    ">\n",
    "> If you haven't done that, nobody's going to do it for you!\n",
    "\n",
    "## Connect to Your Workspace\n",
    "\n",
    "The first thing you need to do is to connect to your workspace using the Azure ML SDK. Let's start by ensuring you still have the latest version installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade azureml-sdk[notebooks,automl,explain]\n",
    "\n",
    "import azureml.core\n",
    "print(\"Ready to use Azure ML\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to connect to your workspace.\n",
    "\n",
    "> **Note**: If the authenticated session with your Azure subscription has expired since you completed the previous exercise, you'll be prompted to reauthenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to work with', ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use *Hyperdrive* to Determine Optimal Parameter Values\n",
    "\n",
    "The remote compute you created is a four-node cluster, and you can take advantage of this to execute multiple experiment runs in parallel. One key reason to do this is to try training a model with a range of different hyperparameter values.\n",
    "\n",
    "Azure ML includes a feature called *hyperdrive* that enables you to randomly try different values for one or more hyperparameters, and find the best performing trained model based on a metric that you specify - such as *Accuracy* or *Area Under the Curve (AUC)*.\n",
    "\n",
    "> **More Information**: For more information about Hyperdrive, see the [Azure ML documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters).\n",
    "\n",
    "Let's run a Hyperdrive experiment on the remote compute you have provisioned. First, we'll create the experiment and its associated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create an experiment\n",
    "hyperdrive_experiment_name = 'diabetes_hyperdrive'\n",
    "hyperdrive_experiment = Experiment(workspace = ws, name = hyperdrive_experiment_name)\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "hyperdrive_experiment_folder = './' + hyperdrive_experiment_name\n",
    "os.makedirs(hyperdrive_experiment_folder, exist_ok=True)\n",
    "\n",
    "print(\"Experiment:\", hyperdrive_experiment.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the Python script our experiment will run in order to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $hyperdrive_experiment_folder/diabetes_training.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Workspace, Dataset, Experiment, Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Set regularization parameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\n",
    "args = parser.parse_args()\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes dataset\n",
    "print(\"Loading Data...\")\n",
    "diabetes = run.input_datasets['diabetes'].to_pandas_dataframe() # Get the training data from the estimator input\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "run.log_image(name = \"ROC\", plot = fig)\n",
    "plt.show()\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/diabetes.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the *Hyperdrive* feature of Azure ML to run multiple experiments in parallel, using different values for the **regularization** parameter to find the optimal value for the data. To help  scale this out, you'll run the Hyperdrive experiment on one of the training compute clusters you created in the previous exercise.\n",
    "\n",
    "> **Note**: If you didn't create it in the previous exercise, don't worry - the code below will create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your cluster\n",
    "cluster_name = \"aml-compute1\"\n",
    "\n",
    "# Get the existing cluster\n",
    "try:\n",
    "    compute1 = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Create it if it doesn't exist\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS2_V2', max_nodes=4)\n",
    "    compute1 = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute1.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to run the experiment.\n",
    "\n",
    "This will take some time to run. First Azure ML must initialize the compute cluster and prepare the Python environment. Then, the experiment will launch a child run for each hyperparameter value combination to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn # We'll use the SKLearn estimator\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        # There's only one parameter, so grid sampling will try each value - with multiple parameters it would try every combination\n",
    "        '--regularization': choice(0.001, 0.005, 0.01, 0.05, 0.1, 1.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set evaluation policy to stop poorly performing training runs early\n",
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "\n",
    "# Get the training dataset\n",
    "diabetes_ds = ws.datasets.get(\"diabetes dataset\")\n",
    "\n",
    "# Create an estimator that uses the remote compute\n",
    "hyper_estimator = SKLearn(source_directory=hyperdrive_experiment_folder,\n",
    "                           inputs=[diabetes_ds.as_named_input('diabetes')], # Pass the dataset as an input\n",
    "                           compute_target = compute1,\n",
    "                           conda_packages=['pandas','ipykernel','matplotlib'], #The estimator already includes scikit-learn\n",
    "                           pip_packages=['azureml-sdk','argparse','pyarrow'],\n",
    "                           entry_script='diabetes_training.py')\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(estimator=hyper_estimator, \n",
    "                          hyperparameter_sampling=params, \n",
    "                          policy=policy, \n",
    "                          primary_metric_name='AUC', \n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=6,\n",
    "                          max_concurrent_runs=4)\n",
    "\n",
    "\n",
    "# Run the experiment\n",
    "hyperdrive_run = hyperdrive_experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the experiment run status in the widget above. You can also view the main Hyperdrive experiment run and its child runs in the [Azure ML Studio web interface](https://ml.azure.com).\n",
    "\n",
    "> **Tip**: While the experiment is running, skip ahead to the **Use *Automated ML* to Find the Best Model for your Data** task below. Then, after you've started the Auto ML experiment, you can come back here and wait for the status of the hyperdrive experiment in the widget above to be **Completed**.\n",
    "\n",
    "You can find the best model based on the performance metric you specified (in this case, the one with the best AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperdrive_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_hyperdrive_run_metrics = best_hyperdrive_run.get_metrics()\n",
    "hyperdrive_parameter_values = best_hyperdrive_run.get_details() ['runDefinition']['arguments']\n",
    "\n",
    "print('Best Run Id: ', best_hyperdrive_run.id)\n",
    "print(' -AUC:', best_hyperdrive_run_metrics['AUC'])\n",
    "print(' -Accuracy:', best_hyperdrive_run_metrics['Accuracy'])\n",
    "print(' -Regularization Rate:',hyperdrive_parameter_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've found the best run, we can register the model it trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "# Register model\n",
    "best_hyperdrive_run.register_model(model_path='outputs/diabetes.pkl', model_name='diabetes', tags={'Training context':'Hyperdrive'}, properties={'AUC': best_hyperdrive_run_metrics['AUC'], 'Accuracy': best_hyperdrive_run_metrics['Accuracy']})\n",
    "\n",
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip**: If you previously skipped ahead to start the Auo ML experiment, scroll down and view its widget to await its completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use *Automated ML* to Find the Best Model for your Data\n",
    "\n",
    "Hyperparameter tuning has helped us find the optimal regularization rate for our logistic regression model, but we might get better results by trying a different algorithm, and by performing some basic feature-engineering, such as scaling numeric feature values. You could just create lots of different training scripts that apply various scikit-learn algorithms, and try them all until you find the best result; but Azure ML provides a feature called *Automated Machine Learning* (or *Auto ML*) that can do this for you.\n",
    "\n",
    "> **Note**: You can use the Auto ML wizard in the [Azure ML Studio web interface](https://ml.azure.com) to submit an Auto ML experiment, or you can initiate Auto ML using the Azure ML SDK. The SDK gives you greater control over the settings for the Auto ML experiment, but the visual interface is easier to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to create a training script (Auto ML will do that for you), but you do need to load the training data.\n",
    "\n",
    "There are a few ways you can do this, but one of the easiest is to use a dataset. In this case, we'll use the diabetes dataset, but we'll also split it into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training dataset\n",
    "diabetes_ds = ws.datasets.get(\"diabetes dataset\")\n",
    "train_ds, test_ds = diabetes_ds.random_split(percentage=0.7, seed=123)\n",
    "print(\"Data ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, you can scale out the experiment by running it on a training compute cluster, so let's get a reference to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your cluster\n",
    "cluster_name = \"aml-compute2\"\n",
    "\n",
    "# Get the existing cluster\n",
    "try:\n",
    "    compute2 = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Create it if it doesn't exist\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS2_V2', max_nodes=4)\n",
    "    compute2 = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute2.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to configure the Auto ML experiment. To do this, you'll need a run configuration that includes the required packages for the experiment environment, and a set of configuration settings that tells Auto ML how many options to try, which metric to use when evaluating models, and so on.\n",
    "\n",
    "> **More Information**: For more information about options when using Auto ML, see the [Azure ML documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import time\n",
    "import logging\n",
    "\n",
    "\n",
    "automl_run_config = RunConfiguration(framework=\"python\")\n",
    "automl_run_config.environment.docker.enabled = True\n",
    "\n",
    "automl_settings = {\n",
    "    \"name\": \"Diabetes_AutoML_{0}\".format(time.time()),\n",
    "    \"iteration_timeout_minutes\": 10,\n",
    "    \"iterations\": 6,\n",
    "    \"primary_metric\": 'AUC_weighted',\n",
    "    \"preprocess\": False,\n",
    "    \"max_concurrent_iterations\": 4\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='classification',\n",
    "                             debug_log='automl_errors.log',\n",
    "                             compute_target=compute2,\n",
    "                             run_configuration=automl_run_config,\n",
    "                             training_data = train_ds,\n",
    "                             validation_data = test_ds,\n",
    "                             label_column_name='Diabetic',\n",
    "                             model_explainability=True,\n",
    "                             **automl_settings,\n",
    "                             )\n",
    "\n",
    "print(\"Ready for Auto ML run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we're ready to go. Let's start the Auto ML run.\n",
    "\n",
    "This will take a significant amount of time. Eventually, a widget showing the run details will be displayed. You can monitor the compute and experiment status in the [Azure ML Studio web interface](https://ml.azure.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "automl_experiment = Experiment(ws, 'diabetes_automl')\n",
    "automl_run = automl_experiment.submit(automl_config)\n",
    "RunDetails(automl_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip**: If you skipped ahead to start the Auto ML experiment while the Hyperdrive experiment is running, go back now and await the Hyperdrive experiment's completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the output of the experiment in the widget; and when all of the runs have finished, click the run that produced the best result to see its details.\n",
    "\n",
    "You can also view the AutoML experiment run and the models it trained in the [Azure ML Studio web interface](https://ml.azure.com). On the **Automated ML** page, click the **Run ID** of the latest experiment run, and then on the **Models** tab you can view the details for each model.\n",
    "\n",
    "Let's get the best run and the model that was generated (you can ignore any warnings about Azure ML package versions that might appear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_automl_run, fitted_model = automl_run.get_output()\n",
    "print(best_automl_run)\n",
    "print(fitted_model)\n",
    "best_automl_run_metrics = best_automl_run.get_metrics()\n",
    "for metric_name in best_automl_run_metrics:\n",
    "    metric = best_automl_run_metrics[metric_name]\n",
    "    print(metric_name, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the options you used was to include model *explainability*. This uses a test dataset to evaluate the importance of each feature. You can retrieve this information from the run.\n",
    "\n",
    "> **Note**: The model explanation is generated by a separate experiment run that may take some additional time to finish. If running the cell below results in an error, wait a few minutes and try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run(best_automl_run)\n",
    "engineered_explanations = client.download_model_explanation(raw=True)\n",
    "feature_importances = engineered_explanations.get_feature_importance_dict()\n",
    "\n",
    "# Overall feature importance (the Feature value is the column index in the training data)\n",
    "print(\"Feature\\tImportance\")\n",
    "for key, value in feature_importances.items():\n",
    "    print(key, \"\\t\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, having found the best performing model, you can register it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "# Register model\n",
    "best_automl_run.register_model(model_path='outputs/model.pkl', model_name='diabetes', tags={'Training context':'Auto ML'}, properties={'AUC': best_automl_run_metrics['AUC_weighted'], 'Accuracy': best_automl_run_metrics['accuracy']})\n",
    "\n",
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've seen several ways to leverage the high-scale compute capabilities of the cloud to experiment with model training and find the best performing model for your data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
